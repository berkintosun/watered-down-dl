 soft attention, which multiplies features with a (soft) mask of values between zero and one
 hard attention, when those values are constrained to be exactly zero or one.


**Useful Resources**
https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html