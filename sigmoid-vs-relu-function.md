Changing from sigmoid to RELU function for activation function, gradient descent will work much faster.