Word2Vec is the name given to a class of neural network models that, given an unlabelled
training corpus, produce a vector for each word in the corpus that encodes its
semantic information. 


**Useful Sources**
http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf
